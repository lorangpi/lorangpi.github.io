<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Pierrick Lorang — Research</title>
  <meta name="description" content="Pierrick Lorang — Neuro-Symbolic Robotics, HRI, RL. Tufts University & Austrian Institute of Technology." />
  <meta property="og:title" content="Pierrick Lorang — Research" />
  <meta property="og:description" content="Neuro-symbolic robotics, few-shot imitation, continual learning, and foundation-model assisted robot learning." />
  <meta name="author" content="Pierrick Lorang" />
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;700&display=swap" rel="stylesheet">
  <style>
    :root{
      --bg: #f5f7fa;
      --card: #ffffff;
      --muted: #6b7280;
      --accent: #2563eb;
      --accent-2: #7c3aed;
      --text: #0f172a;
      --glass: rgba(255,255,255,0.6);
    }
    *{box-sizing:border-box}
    html,body{height:100%}
    body{
      font-family: 'Inter', system-ui, -apple-system, 'Segoe UI', Roboto, 'Helvetica Neue', Arial;
      background: linear-gradient(180deg,var(--bg) 0%, #eef2f6 100%);
      color:var(--text);
      margin:0;
      -webkit-font-smoothing:antialiased;
      -moz-osx-font-smoothing:grayscale;
      line-height:1.5;
    }
    .wrap{max-width:1100px;margin:28px auto;padding:24px}
    header.site-header{display:flex;gap:20px;align-items:center;padding:22px;border-radius:12px;background:var(--card);box-shadow:0 6px 24px rgba(16,24,40,0.06)}
    .avatar{width:110px;height:110px;border-radius:50%;display:block;object-fit:cover;flex-shrink:0;border-radius:50%;} 
    h1{font-size:1.6rem;margin:0}
    .meta{color:var(--muted);font-size:0.95rem}
    .top-links{display:flex;gap:10px;margin-top:12px;flex-wrap:wrap}
    .btn{display:inline-block;padding:8px 12px;border-radius:8px;font-weight:600;text-decoration:none;border:1px solid transparent}
    .btn.ghost{background:transparent;border:1px solid rgba(16,24,40,0.06);color:var(--text)}
    .btn.primary{background:var(--accent);color:white}

    nav{margin-top:18px;background:transparent}
    nav ul{display:flex;gap:18px;list-style:none;padding:0;margin:0}
    nav a{color:var(--muted);text-decoration:none;font-weight:600}
    nav a:hover{color:var(--accent)}

    main{display:block;gap:22px;margin-top:18px}
    @media (max-width:1000px){main{grid-template-columns:1fr}}

    section.card{background:var(--card);padding:20px;border-radius:12px;box-shadow:0 6px 18px rgba(16,24,40,0.04)}
    h2.section-title{font-size:1.1rem;margin:0 0 8px}
    p.lead{color:var(--muted);margin-bottom:12px}

    .research-list{display:grid;gap:10px}
    .chip{display:inline-block;padding:6px 10px;border-radius:999px;background:#f3f4f6;color:var(--muted);font-weight:600;font-size:0.85rem}

    .pub{padding:14px;border-radius:10px;background:linear-gradient(180deg,#ffffff,#fbfdff);border:1px solid rgba(37,99,235,0.06);display:flex;gap:12px;align-items:flex-start}
    .pub .meta-block{flex:1}
    .pub h3{margin:0;font-size:1rem}
    .pub .byline{color:var(--muted);font-size:0.9rem;margin-top:6px}
    .pub .venue{color:var(--muted);font-style:italic;margin-top:8px}
    .pub .actions{display:flex;gap:8px;margin-top:10px}
    .pub .actions a{font-weight:600;text-decoration:none;padding:6px 8px;border-radius:8px;border:1px solid rgba(37,99,235,0.08)}

    .media-grid{display:flex;gap:10px;margin-top:12px}
    .media-grid img{width:440px;height:240px;object-fit:cover;border-radius:8px;border:1px solid rgba(15,23,42,0.04)}
    .media-grid {
        display: grid;
        grid-template-columns: repeat(2, minmax(0, 1fr));
        gap: 10px;
        margin-top: 10px;
    }

    aside.sidebar .card{margin-bottom:12px}
    .resources a{display:block;color:var(--accent);text-decoration:none;font-weight:600;margin-bottom:8px}

    footer{margin-top:22px;text-align:center;color:var(--muted);font-size:0.9rem}

    /* small helper */
    .badge{display:inline-block;padding:4px 8px;border-radius:6px;font-weight:700;font-size:0.78rem}
    .badge.published{background:#ecfdf5;color:#065f46;border:1px solid rgba(6,95,70,0.06)}
    .badge.accepted{background:#eff6ff;color:#1e3a8a;border:1px solid rgba(30,58,138,0.06)}
  </style>
</head>
<body>
  <div class="wrap">
    <header class="site-header">
      <img src="assets/profile.jpg" alt="Pierrick Lorang" class="avatar" />
      <div style="flex:1">
        <h1>Pierrick Lorang</h1>
        <div class="meta">Ph.D. Candidate — Mechanical Engineering & Human-Robot Interaction<br>Tufts University & Austrian Institute of Technology</div>
        <div class="top-links">
          <a class="btn primary" href="mailto:pierrick.lorang@gmail.com">Email</a>
          <a class="btn ghost" href="assets/cv.pdf" target="_blank">CV (PDF)</a>
          <a class="btn ghost" href="https://github.com/lorangpi" target="_blank">GitHub</a>
          <a class="btn ghost" href="https://scholar.google.com/citations?user=fuj2TwsAAAAJ&hl=fr" target="_blank">Google Scholar</a>
          <a class="btn ghost" href="https://www.linkedin.com/in/pierrick-lorang-099423159/" target="_blank">LinkedIn</a>
        </div>
        <div style="margin-top:12px;color:var(--muted);font-size:0.95rem;">
          <strong>Email:</strong> pierrick.lorang@gmail.com<br>
          <strong>Laboratories:</strong> Human-Robot Interaction @ Tufts University, Medford, MA & Complex Dynamical Systems @ the Austrian Institute of Technology, Vienna, Austria<br>
          <strong>Phone:</strong> +33 769 109 816
        </div>
      </div>
    </header>

    <nav>
      <ul>
        <li><a href="#about">About</a></li>
        <li><a href="#research">Research</a></li>
        <li><a href="#publications">Publications</a></li>
        <li><a href="#media">Media</a></li>
        <li><a href="#contact">Contact</a></li>
      </ul>
    </nav>

    <main>
      <div>
        <section id="about" class="card">
          <h2 class="section-title">About</h2>
          <p class="lead">I develop neuro-symbolic architectures for robots that enable few-shot imitation, long-horizon planning, and continual adaptation in open-world environments. My research sits at the intersection of symbolic reasoning, deep learning, and control.</p>

          <div class="research-list">
            <div class="chip">Neuro-Symbolic AI</div>
            <div class="chip">Few-Shot Imitation</div>
            <div class="chip">Hierarchical RL</div>
            <div class="chip">Continual Learning</div>
            <div class="chip">Foundation Models for Robotics</div>
          </div>
        </section>

        <section id="research" class="card" style="margin-top:16px">
            <h2 class="section-title">Research Overview</h2>

            <p class="lead">
                Current focus: integrating foundation visual-language models with symbolic planners
                and data-efficient motor primitives to scale from simulation to real machines.
            </p>

            <h3 style="margin-top:12px">Projects</h3>
            <ul>
                <li><strong>Foundation Model-Assisted Robot Learning</strong>:
                    automatic symbol and skill discovery from demonstrations.</li>
                <li><strong>LLM-Guided Symbolic Planning</strong>:
                    bridging LLMs and symbolic planners for adaptable behaviors.</li>
                <li><strong>Industrial Deployments</strong>:
                    collaboration with the Austrian Institute of Technology for real-machine trials.</li>
            </ul>

            <p class="demo-text">
                Demonstration of a neuro-symbolic agent learning to
                <em>cook an eggplant</em> and <em>clean a table</em> from
                one-shot demonstrations by building on priors from foundation models and control policies.
                The agent distills knowledge from a VLM to detect objects and actions, abstracts trajectory
                checkpoints, reprojects them to novel objects via control policies, imitates low-level actions
                using diffusion-based learning, and discovers symbolic operators and task abstractions via a
                satisfiability solver.
            </p>

            <div class="media-grid media-grid-2x2">
                <img src="assets/eggplant1.gif" alt="Front view of the agent cooking an eggplant">
                <img src="assets/eggplant2.gif" alt="Agent view of the agent cooking an eggplant">
                <img src="assets/table1.gif" alt="Front view of the agent cleaning a tablhttps://lorangpi.github.io/e">
                <img src="assets/table2.gif" alt="Agent view of the agent cleaning a table">
            </div>
        </section>



        <section id="publications" class="card" style="margin-top:16px">
          <h2 class="section-title">Selected Publications</h2>

          <!-- CoRL 2025 -->
          <article class="pub" aria-labelledby="corl-title">
            <div class="meta-block">
              <h3 id="corl-title">Few-Shot Neuro-Symbolic Imitation Learning for Long-Horizon Planning and Acting <span class="badge published">CoRL 2025</span></h3>
              <div class="byline">Pierrick Lorang, Hong Lu, Johannes Huemer, Patrik Zips, Matthias Scheutz</div>
              <div class="venue">Conference on Robot Learning (CoRL), 2025</div>
              <div class="actions">
                <a href="https://arxiv.org/abs/2508.21501">PDF</a>
                <a href="https://www.youtube.com/watch?v=E8slaN81oAA&t=6s">Video</a>
              </div>

              <p class="pub-desc">Imitation learning enables intelligent systems to acquire complex behaviors with minimal supervision, but existing methods often target short-horizon skills, require large datasets, and fail to generalize under distribution shifts. We propose a novel neuro-symbolic framework that jointly learns continuous control policies and symbolic domain abstractions from a handful of skill demonstrations. The method constructs a graph-based task abstraction, discovers symbolic rules using an Answer Set Programming solver, and trains diffusion-policy controllers for low-level actions. A high-level oracle filters task-relevant information to keep each controller focused on a minimal observation-action set. This graph-based abstraction captures non-spatial and temporal relations that clustering or purely data-driven techniques miss in limited-data regimes. We validate across six domains — multi-arm manipulation, stacking, kitchen, assembly, Towers of Hanoi, and an automated-forklift domain — demonstrating high data-efficiency (as few as five demonstrations), strong zero- and few-shot generalization, and interpretable decision-making.</p><div class="media-grid" style="margin-top:10px">
                <img src="assets/corl_fewshot_demo1.gif" alt="CoRL demo 1: few-shot imitation" />
                <img src="assets/corl_fewshot_demo2.gif" alt="CoRL demo 2: planning" />
              </div>
            </div>
          </article>

          <!-- IROS 2024 -->
          <article class="pub" style="margin-top:12px" aria-labelledby="iros-title">
            <div class="meta-block">
              <h3 id="iros-title">A Framework for Neurosymbolic Goal-Conditioned Continual Learning in Open World Environments <span class="badge accepted">IROS 2024</span></h3>
              <div class="byline">Pierrick Lorang, Shivam Goel, Yash Shukla, et al.</div>
              <div class="venue">IEEE/RSJ IROS, 2024</div>
              <div class="actions">
                <a href="https://ieeexplore.ieee.org/abstract/document/10801627">PDF</a>
              </div>

              <p class="pub-desc">In dynamic open-world environments, agents face sudden and unpredictable novelties that hinder Task and Motion Planning. We propose a TAMP architecture that tightly integrates symbolic planning with reinforcement learning to enable autonomous adaptation without human guidance. The approach uses symbolic goal representations within a goal-conditioned learning framework and employs planner-guided goal identification to handle abrupt changes where traditional RL and re-planning fail. Sequential novelty-injection experiments demonstrate faster convergence and more robust long-horizon performance compared to standard baselines, illustrating the method's practical utility for real-world robotic systems.</p><div class="media-grid" style="margin-top:10px">
                <img src="assets/iros_continual_demo.gif" alt="IROS demo: continual learning" />
              </div>
            </div>
          </article>

          <!-- More papers condensed -->
          <div style="margin-top:14px">

              <div style="margin-bottom:10px">
                <p style="margin:6px 0"><strong>Curiosity-Driven Imagination: Discovering Plan Operators and Learning Associated Policies for Open-World Adaptation</strong> — ICRA 2025 (preprint)</p>
                <p class="pub-desc">Adapting quickly to dynamic, uncertain “open world” environments remains a major robotics challenge. We introduce a hybrid planning-and-learning system that pairs a learned stochastic low-level model (trained with an Intrinsic Curiosity Module to drive exploration) with a high-level symbolic planner that captures abstract transitions via operators. The agent plans in an "imaginary" symbolic space to generate reward machines and guide learning. In robotic manipulation tasks subject to sequential novelty injections, this curiosity-driven imagination approach converges faster and outperforms state-of-the-art hybrid TAMP methods, showing superior adaptability and data-efficiency.</p>
                    <div class="actions">
                        <a href="https://arxiv.org/abs/2503.04931">PDF</a>
                    </div>
              </div>

              <div style="margin-bottom:10px">
                <p style="margin:6px 0"><strong>Adapting to the "Open World": The Utility of Hybrid Hierarchical Reinforcement Learning and Symbolic Planning</strong> — ICRA 2024</p>
                <p class="pub-desc">Open-world robotic tasks (e.g., autonomous driving) present unpredictable events that disrupt controllers. Neural RL methods struggle to adapt and suffer from catastrophic forgetting; hybrid planning+RL approaches help but adapt slowly. We propose an enhanced hybrid system with nested hierarchical action abstraction that reuses learned skills across abstraction levels and leverages symbolic planning to select and compose them efficiently. Experiments show faster adaptation, improved generalization, and greater robustness when multiple environmental novelties occur simultaneously, outperforming state-of-the-art RL and hybrid baselines.</p>
                    <div class="actions">
                        <a href="https://ieeexplore.ieee.org/abstract/document/10611594">PDF</a>
                    </div>
              </div>

              <div style="margin-bottom:10px">
                <p style="margin:6px 0"><strong>A Neurosymbolic Cognitive Architecture Framework for Handling Novelties in Open Worlds</strong> — Artificial Intelligence, 2024</p>
                <p class="pub-desc">This journal article describes a large-scale neurosymbolic cognitive architecture that combines symbolic planning, counterfactual reasoning, reinforcement learning, and deep vision to detect and accommodate concealed novelties in Minecraft-like environments. The framework enables agents to maintain task performance despite unseen changes and is evaluated extensively on diverse novelty scenarios.</p>
                    <div class="actions">
                        <a href="https://www.sciencedirect.com/science/article/abs/pii/S000437022400047X">PDF</a>
                    </div>
              </div>

              <div style="margin-bottom:10px">
                <p style="margin:6px 0"><strong>Speeding-up Continual Learning Through Information Gains in Novel Experiences</strong> — PRL Workshop at IJCAI-2022</p>
                <p class="pub-desc">We introduce an information-gain-based signal to prioritize learning from novel experiences and speed up continual learning. By estimating the information contribution of unexpected transitions, agents focus updates on experiences that maximize learning progress and reduce catastrophic forgetting. Empirical results in simulated continual-learning scenarios show faster adaptation and improved retention compared to standard continual-RL baselines.</p>
                    <div class="actions">
                        <a href="https://prl-theworkshop.github.io/prl2022-ijcai/papers/PRL2022_paper_26.pdf">PDF</a>
                    </div>
              </div>

              <hr />

              <div style="margin-top:10px">
                <p style="margin:6px 0"><strong>Comparing the Cost and Performance of Vision-Language-Action Models Against Classical Neuro-Symbolic Approaches</strong> — Adv. Robotics Research Journal, Under Review (2026)</p>
                <p class="pub-desc">There is much excitement around Vision-Language-Action (VLA) models, but their suitability for structured, long-horizon robotic tasks remains unclear — especially given the large training and inference energy costs. This manuscript provides a rigorous empirical evaluation comparing a fine-tuned OpenPI VLA with a neuro-symbolic architecture that combines PDDL-based symbolic planning and diffusion-model controllers. Using a custom Towers-of-Hanoi benchmark in Robosuite, we measure success rate, training/fine-tuning time, inference time, and energy consumption across task variants. Results indicate that neuro-symbolic approaches achieve substantially higher task success while consuming orders-of-magnitude less energy, suggesting that energy-efficient symbolic methods remain critical for robotic applications.</p>
              </div>

              <div style="margin-top:10px">
                <p style="margin:6px 0"><strong>Neuro-Symbolic Epistemic Agents for POMDP-Based Task Planning</strong> — ICAPS, Under Review (2026)</p>
                <p class="pub-desc">We propose a neurosymbolic epistemic planning framework for POMDP-based task planning that integrates epistemic operators with reinforcement-learning-driven belief updates. The system discovers epistemic operators, maintains compact belief-state abstractions, and uses symbolic planning to guide targeted exploration and disambiguation under uncertainty. Experiments in multi-object manipulation POMDPs show improved sample efficiency and more robust task completion compared to baselines that lack explicit epistemic reasoning.</p>
              </div>

              <div style="margin-top:10px">
                <p style="margin:6px 0"><strong>Build on Priors: Distilling Knowledge from Controls and Foundation Models for Efficient and Adaptive Neuro-Symbolic Architectures</strong> — IEEE RA-L, Under Review (2026)</p>
                <p class="pub-desc">We present a hierarchical knowledge-distillation pipeline for neurosymbolic robotics that leverages priors from multiple layers: high-level symbolic reasoning, transformer-based perception, and low-level motion planners/controllers. By distilling these heterogeneous sources into compact neurosymbolic modules, the approach accelerates learning on real robots, improves adaptability, and preserves interpretability while benefiting from foundation-model perceptual features and established control policies.</p>
              </div>

              <hr />

              <div style="margin-top:12px">
                <p style="margin:6px 0"><strong>Integrating LLMs and Classical Planning for Pallet Logistics: A Case Study</strong> — IFAC, 2026</p>
                <p class="pub-desc">Evaluation of LLMs as planners in a pallet logistics PDDL domain, proposing a hybrid pipeline where LLMs translate language into goal definitions and classical planners ensure executable, optimal plans.</p>
                    <div class="actions">
                        <a href="https://www.sciencedirect.com/science/article/pii/S2405896325016362">PDF</a>
                    </div>
              </div>

              <div style="margin-top:12px">
                <p style="margin:6px 0"><strong>Novelty Adaptation Through Hybrid LLM-Symbolic Planning and LLM-Guided RL</strong> — ICRA, Under Review (2026)</p>
                <p class="pub-desc">A neurosymbolic system where an LLM proposes missing operators and reward scaffolds; symbolic planning generates plans while RL learns low-level controllers for newly discovered operators, showing improved novelty accommodation in tabletop manipulation domains.</p>
              </div>

              <div style="margin-top:12px">
                <p style="margin:6px 0"><strong>Understanding and Explaining Vision-Language-Action Models Through Probing</strong> — In Preparation (2026)</p>
                <p class="pub-desc">Probing OpenVLA representations to extract symbolic object and action states for integration into cognitive architectures. Experiments on LIBERO-spatial tasks show high probing accuracies and enable an integrated DIARC-OpenVLA system for interpretable real-time monitoring.</p>
              </div>

            </div>
        </section>


      </div>

      </main>

    <footer>
      <small>© 2025 Pierrick Lorang — Last updated: November 2025</small>
    </footer>
  </div>
</body>
</html>
